{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12201425,"sourceType":"datasetVersion","datasetId":7685774}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T01:06:06.478306Z","iopub.execute_input":"2025-07-05T01:06:06.478590Z","iopub.status.idle":"2025-07-05T01:06:06.817532Z","shell.execute_reply.started":"2025-07-05T01:06:06.478568Z","shell.execute_reply":"2025-07-05T01:06:06.816853Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/asimov/asimov_cleaned.txt\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n# ====================================\n# create a class to tokenize and encode input text, and load them into input and target tensors in batches\n# txt         > the input text to tokenize\n# tokenizer   > the tokenizer used to convert text into token IDs\n# max_length  > the fixed size of each input window (number of tokens per chunk)\n# stride      > the number of tokens to shift the window forward for the next chunk\n#\n# Together, these parameters define a sliding window over the tokenized text.\n# At each step, a chunk of token IDs of length max_length is extracted,\n# and the window moves forward by 'stride' tokens to produce the next chunk.\n# This allows overlapping input-target pairs to be generated for training language models\n# ======================================\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        \n        # Initialize input and target list\n        self.input_ids = [] \n        self.target_ids = []\n\n        token_ids = tokenizer.encode(txt) \n\n        # Iterate over the tokenized sequence in overlapping chunks of max_length, stepping by stride\n        for i in range(0, len(token_ids) - max_length, stride):\n            # Extract a chunk of token IDs starting at position i, with a total length of max_length\n            input_chunk = token_ids[i: i + max_length]\n\n            # Extract the target chunk starting one token ahead of the input_chunk — i.e., shifted by 1 position.\n            # For example, if input_chunk = [1, 2, 3, 4], then target_chunk = [2, 3, 4, 5]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n            \n    # Returns the length of input id list\n    def __len__(self):\n        return len(self.input_ids)\n\n    # Returns the input and target tokens in position idx\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T01:06:11.072200Z","iopub.execute_input":"2025-07-05T01:06:11.072617Z","iopub.status.idle":"2025-07-05T01:06:11.080088Z","shell.execute_reply.started":"2025-07-05T01:06:11.072593Z","shell.execute_reply":"2025-07-05T01:06:11.078940Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import tiktoken \n\n#=====================================\n# Creates a PyTorch DataLoader for training\n# Args:\n# - txt (str): The raw input text.\n# - batch_size (int): Number of (input, target) pairs per batch. Default is 4.\n# - max_length (int): The fixed window size (in tokens) for each input chunk. Default is 256.\n# - stride (int): The number of tokens to shift the sliding window at each step. Controls overlap. Default is 128.\n# - shuffle (bool): Whether to shuffle the dataset between epochs. Default is True.\n# - drop_last (bool): Whether to drop the last batch if it's smaller than batch_size. Default is True.\n# - num_workers (int): Number of subprocesses for data loading. Set to 0 for main-thread loading. Default is 0.\n\n#Returns:\n# - DataLoader: A PyTorch DataLoader yielding batches of (input_ids, target_ids) tensors for training.\n#=====================================\n\ndef create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n    # intializes gpt2 tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create a custom dataset that slices tokenized text into overlapping input-target pairs\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n     \n    # Wrap the dataset in a PyTorch DataLoader for batch processing\n    dataloader = DataLoader( \n        dataset,\n        batch_size = batch_size,\n        shuffle = shuffle,\n        drop_last = drop_last,\n        num_workers = num_workers\n    )\n    return(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T01:07:50.424264Z","iopub.execute_input":"2025-07-05T01:07:50.424571Z","iopub.status.idle":"2025-07-05T01:07:50.431047Z","shell.execute_reply.started":"2025-07-05T01:07:50.424547Z","shell.execute_reply":"2025-07-05T01:07:50.429873Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import re\n\n# Load cleaned Asimov corpus\nwith open(\"/kaggle/input/asimov/asimov_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\nmax_length = 4 # The fixed window size (in tokens) for each training chunk\noutput_dim = 256 # # Dimensionality of each token embedding vector\nvocab_size = 50527 # Size of GPT2 tokenizer vocabulary \n\n\n# Create a DataLoader from raw text using fixed-length token chunks\n# No overlap between chunks (stride = max_length), and no shuffling\ndataloader = create_dataloader_v1(raw_text,\n                                  batch_size=8,\n                                  max_length=max_length,\n                                  stride=max_length,\n                                  shuffle=False)\n\ndata_iter = iter(dataloader) # Get an iterator over the DataLoader\ninputs, targets = next(data_iter) # Fetch the first batch of (input_ids, target_ids)\nprint(\"Input tokens:\", inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T01:12:43.827770Z","iopub.execute_input":"2025-07-05T01:12:43.828113Z","iopub.status.idle":"2025-07-05T01:12:57.337380Z","shell.execute_reply.started":"2025-07-05T01:12:43.828091Z","shell.execute_reply":"2025-07-05T01:12:57.336327Z"}},"outputs":[{"name":"stdout","text":"Input tokens: tensor([[ 7003,   314,   423,  3194],\n        [  625,   257,  3470,   290],\n        [ 8208,  3835,    11,   319],\n        [ 2048,   790,  2426,   422],\n        [37145,   284, 22197,   290],\n        [  422, 19473,   284, 35704],\n        [   11,   340,   318,  2192],\n        [  355,   257,  3783, 10165]])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"torch.manual_seed(123) # Manually set seed for reproducibility\n\n# Create a token embedding layer: maps token IDs to 256-dimensional vectors\n# vocab_size = number of unique token IDs, output_dim = size of each embedding vector\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n\n# Look up the embeddings for each token ID in the input batch\n# Resulting shape: [batch_size, max_length, output_dim] → e.g., [8, 4, 256]\ntoken_embeddings = token_embedding_layer(inputs) # Align the token embeddigns to the input tensors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T01:15:32.176744Z","iopub.execute_input":"2025-07-05T01:15:32.177629Z","iopub.status.idle":"2025-07-05T01:15:32.275755Z","shell.execute_reply.started":"2025-07-05T01:15:32.177598Z","shell.execute_reply":"2025-07-05T01:15:32.274810Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"context_length = max_length\n\n# Create a positional embedding layer:\n# One 256-dimensional vector for each position in the sequence (0 to max_length - 1)\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\nprint(\"position embedding layer: \", pos_embedding_layer.weight)\n\n# Generate position indices: [0, 1, ..., max_length - 1]\n# and look up the corresponding positional embeddings\npos_embeddings = pos_embedding_layer(torch.arange(context_length))\nprint(\"position embeddings: \", pos_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T01:23:43.965934Z","iopub.execute_input":"2025-07-05T01:23:43.966530Z","iopub.status.idle":"2025-07-05T01:23:43.975085Z","shell.execute_reply.started":"2025-07-05T01:23:43.966499Z","shell.execute_reply":"2025-07-05T01:23:43.974172Z"}},"outputs":[{"name":"stdout","text":"position embedding layer:  Parameter containing:\ntensor([[-0.8648, -0.7682, -0.6053,  ..., -0.2497, -1.5555,  2.3629],\n        [-0.5688, -1.7592,  0.8757,  ...,  2.8921, -1.3788,  0.3624],\n        [ 1.0010,  0.1634, -1.5274,  ...,  1.6374, -1.0874, -0.4874],\n        [ 0.8589, -0.0860, -1.1566,  ...,  1.7970, -0.1550, -0.5910]],\n       requires_grad=True)\nposition embeddings:  tensor([[-0.8648, -0.7682, -0.6053,  ..., -0.2497, -1.5555,  2.3629],\n        [-0.5688, -1.7592,  0.8757,  ...,  2.8921, -1.3788,  0.3624],\n        [ 1.0010,  0.1634, -1.5274,  ...,  1.6374, -1.0874, -0.4874],\n        [ 0.8589, -0.0860, -1.1566,  ...,  1.7970, -0.1550, -0.5910]],\n       grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"input_embeddings = token_embeddings + pos_embeddings\ninput_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T01:23:51.553966Z","iopub.execute_input":"2025-07-05T01:23:51.554276Z","iopub.status.idle":"2025-07-05T01:23:51.566304Z","shell.execute_reply.started":"2025-07-05T01:23:51.554253Z","shell.execute_reply":"2025-07-05T01:23:51.565424Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"tensor([[[-0.7690,  0.4648, -1.5057,  ..., -2.9360, -1.1125,  2.2660],\n         [-0.2512, -0.9052,  3.0786,  ...,  4.5520, -1.6124,  0.9986],\n         [ 0.1105, -0.9019, -1.8289,  ...,  4.0203, -1.3229,  0.6001],\n         [ 0.4481, -1.1438,  0.2410,  ...,  2.3520, -0.3295, -2.9051]],\n\n        [[-0.0968, -0.1298, -0.3866,  ..., -0.0585, -2.9282,  1.8056],\n         [-0.4498, -0.9992, -0.0549,  ...,  2.8927, -0.5182, -1.0074],\n         [ 2.4654, -0.1377, -0.4795,  ...,  3.0215, -0.5710,  0.7335],\n         [-0.4648,  1.0875, -2.6483,  ...,  2.9446,  0.3891,  0.1538]],\n\n        [[ 1.4138, -0.0513, -1.0795,  ...,  0.6437, -1.2911,  1.8119],\n         [-0.2599, -0.8302,  0.6009,  ...,  4.1686, -2.6684,  1.7722],\n         [ 2.8066, -0.8430, -1.3692,  ...,  1.8753, -2.2713, -0.8053],\n         [-1.0595, -1.3138, -1.5150,  ...,  0.3647, -1.0643, -1.4184]],\n\n        ...,\n\n        [[-0.3777, -0.9435, -1.5514,  ..., -1.0833, -0.3089,  3.9729],\n         [-0.7133, -0.0279,  0.3054,  ...,  5.0487, -0.9687, -0.3737],\n         [-1.7683, -0.9047,  0.2240,  ...,  1.7836, -3.6434,  1.7743],\n         [ 0.5584, -0.9878,  0.1013,  ...,  2.1139, -0.8623, -2.3845]],\n\n        [[ 0.9408, -1.7747, -0.4471,  ..., -0.0117, -2.7394,  2.0450],\n         [ 0.6097, -2.0837,  0.2316,  ...,  2.4990, -2.4194, -1.0286],\n         [ 0.5451, -1.7267, -3.3086,  ...,  1.6613, -0.6710, -0.9090],\n         [ 1.3284, -1.0438, -2.2131,  ...,  2.5365,  0.1030, -1.9972]],\n\n        [[-1.4912, -0.1163, -1.0214,  ...,  0.1254, -1.1989,  2.4517],\n         [-0.4498, -0.9992, -0.0549,  ...,  2.8927, -0.5182, -1.0074],\n         [ 1.2036,  0.5524, -1.1586,  ...,  1.2931, -2.4692,  1.3440],\n         [-0.0303, -0.4887, -1.5332,  ...,  2.5321,  0.9649, -2.9839]]],\n       grad_fn=<AddBackward0>)"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"","metadata":{}}]}