{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# ðŸ“¦ DATA PREP MODULE\n# ===========================\n# Plan:\n# - read and tokenize data\n# - assign token ids to vocab\n# - create token id to vocab lookups and reverse look ups\n# ===========================\n# Contains:\n# - get_vocab_mappings()\n# - encode_data()\n# - create_training_sequences()\n# - batch_generator()\n# ===========================\n\n\nimport re\n\n# Load cleaned Asimov corpus\nwith open(\"/kaggle/input/asimov/asimov_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n    text = f.read()\n\n# tokenize data using python regex function\npattern = r\"([,\\.;\\?/\\-!_\\\"\\'<>\\(\\)\\[\\]\\{\\}\\*\\+\\=\\&\\%\\$\\#\\@\\~]|\\s+)\" # The regex pattern to split by (comma or semicolon or space)\n\ntokens = [t for t in re.split(pattern, text) if t.strip()]\nvocab = list(dict.fromkeys(tokens)) \nvocab.extend([\"<endoftext>\", \"<|unk|>\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:50:53.569557Z","iopub.execute_input":"2025-07-03T05:50:53.569943Z","iopub.status.idle":"2025-07-03T05:50:55.287457Z","shell.execute_reply.started":"2025-07-03T05:50:53.569917Z","shell.execute_reply":"2025-07-03T05:50:55.286556Z"}},"outputs":[{"name":"stdout","text":"44686\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import re\n\nclass TokenizerV1:\n    def __init__(self, vocab): \n        self.str_to_int = vocab \n        self.int_to_str = {i: s for s, i in vocab.items()}  # Fix syntax\n\n    def encode(self, text):\n        pattern = r\"([,\\.;\\?/\\-!_\\\"\\'<>\\(\\)\\[\\]\\{\\}\\*\\+\\=\\&\\%\\$\\#\\@\\~]|\\s+)\"\n        preprocessed = re.split(pattern, text) # split text using the pattern\n        preprocessed = [item.strip() for item in preprocessed if item.strip()] # strip leading and trailing white spaces from tokens, and only include if the string is no null after trimming\n        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed] # replace any token in preprocessed with unknow if not present in vocab\n        \n        ids = [self.str_to_int[s] for s in preprocessed] # create a list of ids by getting ids from vocab for each s in preprocessed\n        return ids\n        \n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids]) # create a text by joning strings from vocab for each id in ids\n        text = re.sub(r'\\s+([,\\.?\\!\"()\\'])', r'\\1', text) # remove space before punctuation\n        return text\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:52:54.289703Z","iopub.execute_input":"2025-07-03T05:52:54.290015Z","iopub.status.idle":"2025-07-03T05:52:54.297667Z","shell.execute_reply.started":"2025-07-03T05:52:54.289995Z","shell.execute_reply":"2025-07-03T05:52:54.296479Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"tokenizer = TokenizerV1(token_to_id)\ntext = \"chatgot is, I firmly believe, the only kind of education there is.\"\n\nids = tokenizer.encode(text)\n\nprint(ids)\n\ntext1 = tokenizer.decode(ids)\n\nprint(text1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:53:07.451583Z","iopub.execute_input":"2025-07-03T05:53:07.451931Z","iopub.status.idle":"2025-07-03T05:53:07.471453Z","shell.execute_reply.started":"2025-07-03T05:53:07.451905Z","shell.execute_reply":"2025-07-03T05:53:07.470399Z"}},"outputs":[{"name":"stdout","text":"[44685, 22, 10, 1, 7068, 2374, 10, 35, 449, 243, 39, 8661, 436, 22, 32]\n<|unk|> is, I firmly believe, the only kind of education there is.\n","output_type":"stream"}],"execution_count":33}]}