{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12201425,"sourceType":"datasetVersion","datasetId":7685774}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# ðŸ“¦ DATA PREP MODULE\n# ===========================\n# Plan:\n# - read and tokenize data\n# - assign token ids to vocab\n# - create token id to vocab lookups and reverse look ups\n# ===========================\n# Contains:\n# - get_vocab_mappings()\n# - encode_data()\n# - create_training_sequences()\n# - batch_generator()\n# ===========================\n\n\nimport re\n\n# Load cleaned Asimov corpus\nwith open(\"/kaggle/input/asimov/asimov_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n    text = f.read()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T03:24:13.552179Z","iopub.execute_input":"2025-07-04T03:24:13.552521Z","iopub.status.idle":"2025-07-04T03:24:13.606764Z","shell.execute_reply.started":"2025-07-04T03:24:13.552495Z","shell.execute_reply":"2025-07-04T03:24:13.605609Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import re\n\nclass TokenizerV1:\n    def __init__(self, vocab): \n        self.str_to_int = vocab \n        self.int_to_str = {i: s for s, i in vocab.items()}  # Fix syntax\n\n    def encode(self, text):\n        pattern = r\"([,\\.;\\?/\\-!_\\\"\\'<>\\(\\)\\[\\]\\{\\}\\*\\+\\=\\&\\%\\$\\#\\@\\~]|\\s+)\"\n        preprocessed = re.split(pattern, text) # split text using the pattern\n        preprocessed = [item.strip() for item in preprocessed if item.strip()] # strip leading and trailing white spaces from tokens, and only include if the string is no null after trimming\n        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed] # replace any token in preprocessed with unknow if not present in vocab\n        \n        ids = [self.str_to_int[s] for s in preprocessed] # create a list of ids by getting ids from vocab for each s in preprocessed\n        return ids\n        \n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids]) # create a text by joning strings from vocab for each id in ids\n        text = re.sub(r'\\s+([,\\.?\\!\"()\\'])', r'\\1', text) # remove space before punctuation\n        return text\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:52:54.289703Z","iopub.execute_input":"2025-07-03T05:52:54.290015Z","iopub.status.idle":"2025-07-03T05:52:54.297667Z","shell.execute_reply.started":"2025-07-03T05:52:54.289995Z","shell.execute_reply":"2025-07-03T05:52:54.296479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tiktoken\n\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nenc_text = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n\ncontext_size = 4\nenc_sample = enc_text[:50]\nx = enc_sample[:context_size]\ny = enc_sample[1:context_size+1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T03:58:12.335129Z","iopub.execute_input":"2025-07-04T03:58:12.335412Z","iopub.status.idle":"2025-07-04T03:58:13.732087Z","shell.execute_reply.started":"2025-07-04T03:58:12.335394Z","shell.execute_reply":"2025-07-04T03:58:13.731199Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        token_ids = tokenizer.encode(txt)\n        print(len(token_ids))\n        print(max_length)\n\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i: i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n            #print(self.input_ids.shape())\n        #print(\"input ids: \", self.input_ids)\n        #print(\"target ids: \", self.target_ids)\n            \n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T03:58:09.233734Z","iopub.execute_input":"2025-07-04T03:58:09.234092Z","iopub.status.idle":"2025-07-04T03:58:09.241671Z","shell.execute_reply.started":"2025-07-04T03:58:09.234068Z","shell.execute_reply":"2025-07-04T03:58:09.240549Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n    dataloader = DataLoader(\n        dataset,\n        batch_size = batch_size,\n        shuffle = shuffle,\n        drop_last = drop_last,\n        num_workers = num_workers   \n    )\n    return(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T03:58:16.822623Z","iopub.execute_input":"2025-07-04T03:58:16.822943Z","iopub.status.idle":"2025-07-04T03:58:16.828847Z","shell.execute_reply.started":"2025-07-04T03:58:16.822920Z","shell.execute_reply":"2025-07-04T03:58:16.827815Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"#txt = \"erary career I wrote nothing but science fiction stories, for magazine publication onlyâ€”and for minute payment. The thought of actually publishing honest-to-goodness books never entered my essentially humble mind. But the time came when I did begin to produce books, and then I began to gather together the material I had earlier written for magazines. Between 1950 and 1969, ten collections appeared (all of which were published by Doubleday). These contained eighty-five stories (plus four pieces of comic verse) originally intended for, and published in, the science fiction magazines. Nearly a quarter of them came from those first eleven years. For the record, these books are: I, ROBOT (1950) FOUNDATION (1951) FOUNDATION AND EMPIRE (1952) SECOND FOUNDATION (1953) THE MARTIAN WAY AND OTHER STORIES (1955) EARTH IS ROOM ENOUGH (1957) NINE TOMORROWS (1959) THE REST OF THE ROBOTS (1964) ASIMOV'S MYSTERIES (1968) NIGHTFALL AND OTHER STORIES (1969) It might be argued that this was quite enough, but in arguing so, one is omitting the ravenous appetites of my readers (bless them!). I am constantly getting letters requesting lists of ancient stories out of me so that the letter writers can haunt secondhand shops for old magazines. There are people who prepare bibliographies of my science fiction (donâ€™t ask me why) and who want to know all sorts of half-forgotten details concerning them. They even grow distinctly angry when they find that some early stories were never sold and no longer exist. They want those, too, apparently, and seem to think I have negligently destroyed a natural resource. So when Panther Books, in England, and Doubleday suggested that I make a collection of those of my early stories not already collected in the ten books listed above, with the literary history of each, I could resist no further. Everyone who has ever met me knows just how amenable to flattery I am, and if you think I can withstand this kind of flattery for more than half a second (as a rough estimate), you are quite w\"\ndataloader = create_dataloader_v1(text[:5000], batch_size=4, max_length=4, stride=4, shuffle=False)\n\ndata_iter = iter(dataloader)\ninputs, targets = next(data_iter)\nprint(inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T04:03:44.214073Z","iopub.execute_input":"2025-07-04T04:03:44.214347Z","iopub.status.idle":"2025-07-04T04:03:44.228301Z","shell.execute_reply.started":"2025-07-04T04:03:44.214329Z","shell.execute_reply":"2025-07-04T04:03:44.227328Z"}},"outputs":[{"name":"stdout","text":"1111\n4\ntensor([[7003,  314,  423, 3194],\n        [ 625,  257, 3470,  290],\n        [8208, 3835,   11,  319],\n        [2048,  790, 2426,  422]])\n","output_type":"stream"}],"execution_count":64}]}