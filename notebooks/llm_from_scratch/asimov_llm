{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12201425,"sourceType":"datasetVersion","datasetId":7685774}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# ðŸ“¦ DATA PREP MODULE\n# ===========================\n# Plan:\n# - read and tokenize data\n# - assign token ids to vocab\n# - create token id to vocab lookups and reverse look ups\n# ===========================\n# Contains:\n# - get_vocab_mappings()\n# - encode_data()\n# - create_training_sequences()\n# - batch_generator()\n# ===========================\n\n\nimport re\n\n# Load cleaned Asimov corpus\nwith open(\"/kaggle/input/asimov/asimov_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:04:37.020157Z","iopub.execute_input":"2025-07-04T20:04:37.020504Z","iopub.status.idle":"2025-07-04T20:04:37.161842Z","shell.execute_reply.started":"2025-07-04T20:04:37.020470Z","shell.execute_reply":"2025-07-04T20:04:37.160698Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import re\n\nclass TokenizerV1:\n    def __init__(self, vocab): \n        self.str_to_int = vocab \n        self.int_to_str = {i: s for s, i in vocab.items()}  # Fix syntax\n\n    def encode(self, text):\n        pattern = r\"([,\\.;\\?/\\-!_\\\"\\'<>\\(\\)\\[\\]\\{\\}\\*\\+\\=\\&\\%\\$\\#\\@\\~]|\\s+)\"\n        preprocessed = re.split(pattern, text) # split text using the pattern\n        preprocessed = [item.strip() for item in preprocessed if item.strip()] # strip leading and trailing white spaces from tokens, and only include if the string is no null after trimming\n        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed] # replace any token in preprocessed with unknow if not present in vocab\n        \n        ids = [self.str_to_int[s] for s in preprocessed] # create a list of ids by getting ids from vocab for each s in preprocessed\n        return ids\n        \n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids]) # create a text by joning strings from vocab for each id in ids\n        text = re.sub(r'\\s+([,\\.?\\!\"()\\'])', r'\\1', text) # remove space before punctuation\n        return text\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:04:39.790324Z","iopub.execute_input":"2025-07-04T20:04:39.790741Z","iopub.status.idle":"2025-07-04T20:04:39.800770Z","shell.execute_reply.started":"2025-07-04T20:04:39.790703Z","shell.execute_reply":"2025-07-04T20:04:39.799508Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import tiktoken\n\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nenc_text = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n\ncontext_size = 4\nenc_sample = enc_text[:50]\nx = enc_sample[:context_size]\ny = enc_sample[1:context_size+1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:04:42.432678Z","iopub.execute_input":"2025-07-04T20:04:42.433443Z","iopub.status.idle":"2025-07-04T20:04:46.916323Z","shell.execute_reply.started":"2025-07-04T20:04:42.433413Z","shell.execute_reply":"2025-07-04T20:04:46.914660Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3547913539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0menc_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"<|endoftext|>\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcontext_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"],"ename":"NameError","evalue":"name 'text' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        token_ids = tokenizer.encode(txt)\n\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i: i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n            \n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:07:11.863341Z","iopub.execute_input":"2025-07-04T20:07:11.863678Z","iopub.status.idle":"2025-07-04T20:07:11.870977Z","shell.execute_reply.started":"2025-07-04T20:07:11.863621Z","shell.execute_reply":"2025-07-04T20:07:11.869994Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n    dataloader = DataLoader(\n        dataset,\n        batch_size = batch_size,\n        shuffle = shuffle,\n        drop_last = drop_last,\n        num_workers = num_workers   \n    )\n    return(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:04:54.413781Z","iopub.execute_input":"2025-07-04T20:04:54.414278Z","iopub.status.idle":"2025-07-04T20:04:54.420185Z","shell.execute_reply.started":"2025-07-04T20:04:54.414251Z","shell.execute_reply":"2025-07-04T20:04:54.419222Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"max_length = 4\ndataloader = create_dataloader_v1(raw_text,\n                                  batch_size=8,\n                                  max_length=max_length,\n                                  stride=max_length,\n                                  shuffle=False)\n\ndata_iter = iter(dataloader)\ninputs, targets = next(data_iter)\nprint(inputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:07:14.992055Z","iopub.execute_input":"2025-07-04T20:07:14.992351Z","iopub.status.idle":"2025-07-04T20:07:27.041320Z","shell.execute_reply.started":"2025-07-04T20:07:14.992330Z","shell.execute_reply":"2025-07-04T20:07:27.040318Z"}},"outputs":[{"name":"stdout","text":"tensor([[ 7003,   314,   423,  3194],\n        [  625,   257,  3470,   290],\n        [ 8208,  3835,    11,   319],\n        [ 2048,   790,  2426,   422],\n        [37145,   284, 22197,   290],\n        [  422, 19473,   284, 35704],\n        [   11,   340,   318,  2192],\n        [  355,   257,  3783, 10165]])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"output_dim = 256\nvocab_size = 50527\n\ntorch.manual_seed(123)\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\ntoken_embedding_layer\ntoken_embeddings = token_embedding_layer(inputs)\ntoken_embeddings.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:12:21.000980Z","iopub.execute_input":"2025-07-04T20:12:21.001309Z","iopub.status.idle":"2025-07-04T20:12:21.143622Z","shell.execute_reply.started":"2025-07-04T20:12:21.001286Z","shell.execute_reply":"2025-07-04T20:12:21.142559Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"torch.Size([8, 4, 256])"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"context_length = max_length\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\npos_embeddings = pos_embedding_layer(torch.arange(context_length))\nprint(pos_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:17:31.487280Z","iopub.execute_input":"2025-07-04T20:17:31.487562Z","iopub.status.idle":"2025-07-04T20:17:31.498351Z","shell.execute_reply.started":"2025-07-04T20:17:31.487542Z","shell.execute_reply":"2025-07-04T20:17:31.497055Z"}},"outputs":[{"name":"stdout","text":"tensor([[ 0.0981, -0.5413,  0.6534,  ..., -0.1217, -0.8495, -0.0130],\n        [-0.4430,  2.4905, -0.6862,  ..., -0.7371, -1.6232,  0.3666],\n        [ 0.4894,  0.8308,  0.8545,  ...,  0.0654, -1.7289,  0.3837],\n        [ 0.7959,  1.6625,  1.3412,  ..., -1.3240,  0.2213,  0.6629]],\n       grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"input_embeddings = token_embeddings + pos_embeddings\ninput_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T20:19:01.681020Z","iopub.execute_input":"2025-07-04T20:19:01.681336Z","iopub.status.idle":"2025-07-04T20:19:01.691972Z","shell.execute_reply.started":"2025-07-04T20:19:01.681314Z","shell.execute_reply":"2025-07-04T20:19:01.690945Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 0.1939,  0.6918, -0.2469,  ..., -2.8080, -0.4065, -0.1099],\n         [-0.1254,  3.3444,  1.5167,  ...,  0.9228, -1.8567,  1.0028],\n         [-0.4011, -0.2345,  0.5530,  ...,  2.4483, -1.9643,  1.4712],\n         [ 0.3852,  0.6047,  2.7388,  ..., -0.7690,  0.0469, -1.6513]],\n\n        [[ 0.8661,  0.0972,  0.8721,  ...,  0.0695, -2.2222, -0.5703],\n         [-0.3240,  3.2505, -1.6168,  ..., -0.7365, -0.7626, -1.0032],\n         [ 1.9538,  0.5297,  1.9024,  ...,  1.4495, -1.2124,  1.6046],\n         [-0.5277,  2.8359, -0.1506,  ..., -0.1764,  0.7655,  1.4076]],\n\n        [[ 2.3767,  0.1757,  0.1792,  ...,  0.7717, -0.5852, -0.5639],\n         [-0.1341,  3.4195, -0.9610,  ...,  0.5394, -2.9127,  1.7764],\n         [ 2.2949, -0.1756,  1.0127,  ...,  0.3033, -2.9127,  0.0658],\n         [-1.1224,  0.4347,  0.9827,  ..., -2.7563, -0.6879, -0.1645]],\n\n        ...,\n\n        [[ 0.5852, -0.7166, -0.2926,  ..., -0.9554,  0.3971,  1.5970],\n         [-0.5875,  4.2217, -1.2565,  ...,  1.4195, -1.2130, -0.3695],\n         [-2.2800, -0.2373,  2.6059,  ...,  0.2115, -4.2848,  2.6454],\n         [ 0.4954,  0.7607,  2.5991,  ..., -1.0071, -0.4859, -1.1306]],\n\n        [[ 1.9037, -1.5477,  0.8117,  ...,  0.1162, -2.0334, -0.3309],\n         [ 0.7355,  2.1659, -1.3303,  ..., -1.1302, -2.6637, -1.0244],\n         [ 0.0334, -1.0593, -0.9267,  ...,  0.0893, -1.3125, -0.0379],\n         [ 1.2655,  0.7047,  0.2847,  ..., -0.5845,  0.4794, -0.7433]],\n\n        [[-0.5283,  0.1106,  0.2373,  ...,  0.2534, -0.4929,  0.0758],\n         [-0.3240,  3.2505, -1.6168,  ..., -0.7365, -0.7626, -1.0032],\n         [ 0.6920,  1.2198,  1.2234,  ..., -0.2790, -3.1106,  2.2151],\n         [-0.0932,  1.2598,  0.9646,  ..., -0.5889,  1.3413, -1.7300]]],\n       grad_fn=<AddBackward0>)"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"","metadata":{}}]}