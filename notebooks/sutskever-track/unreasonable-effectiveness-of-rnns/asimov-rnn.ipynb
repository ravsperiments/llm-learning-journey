{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12201425,"sourceType":"datasetVersion","datasetId":7685774}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Character-Level RNN: Text Generation from Asimov's *Foundation*\n\nüß† Goal:\nBuild a character-level RNN from scratch using NumPy to model language patterns in Isaac Asimov's *Foundation*.\nThe trained model will generate Asimov-style text one character at a time.\n\nüîÅ Plan:\n\n1. üìñ Load and preprocess text\n   - Read *Foundation* text\n   - Create character-to-index (char2idx) and index-to-character (idx2char) mappings\n   - Encode text as integer sequence\n\n2. üß± Initialize RNN model\n   - Parameters: Wxh, Whh, Why, bh, by\n   - Hidden state size: e.g., 100\n\n3. üîÑ Forward and Backward Pass\n   - Implement one time-step forward: h_t = tanh(Wxh¬∑x_t + Whh¬∑h_{t-1} + bh)\n   - Predict next char logits: y_t = Why¬∑h_t + by\n   - Use softmax + cross-entropy loss\n   - Backpropagate gradients through time (BPTT)\n   - Apply gradient clipping\n\n4. üèãÔ∏è‚Äç‚ôÇÔ∏è Training Loop\n   - Slide a window over text with fixed sequence length (e.g., 25 chars)\n   - Compute loss and gradients\n   - Update parameters via SGD\n\n5. üß™ Sampling Function\n   - Start from a seed character\n   - Sample next character from softmax distribution\n   - Repeat for N characters\n\n6. üìâ Monitoring\n   - Print loss every N steps\n   - Print sample text every 1000 iterations\n\n7. üöÄ [Optional] Try with different corpora (e.g., Sanskrit, Shakespeare)\n\nThis notebook is a stepping stone toward building a Sanskrit name generator using character-level RNNs.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.012839Z","iopub.execute_input":"2025-06-21T06:01:33.013173Z","iopub.status.idle":"2025-06-21T06:01:33.018070Z","shell.execute_reply.started":"2025-06-21T06:01:33.013146Z","shell.execute_reply":"2025-06-21T06:01:33.017077Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ===========================\n# üîÅ RNN FORWARD + BACKWARD MODULE\n# Contains:\n# - rnn_forward_backward(): forward and backward pass of a simple RNN\n# - update_params(): SGD update with optional gradient clipping\n# ===========================\n\n\ndef rnn_forward_backward(x_seq, y_seq, h_prev, params, vocab_size):\n    \"\"\"\n    Perform a forward and backward pass of a vanilla RNN.\n    \n    Args:\n        x_seq (list[int]): Sequence of input character indices.\n        y_seq (list[int]): Sequence of target character indices.\n        h_prev (np.ndarray): Hidden state from previous batch.\n        params (dict): Dictionary of model parameters.\n        vocab_size (int): Size of vocabulary.\n        \n    Returns:\n        dict: Gradients for parameters, final hidden state, and total loss.\n    \"\"\"\n    Wxh, Whh, Why = params['Wxh'], params['Whh'], params['Why']\n    bxh, bhy = params['bxh'], params['bhy']\n\n    seq_len = len(x_seq)\n    h, y_pred, p, X = {}, {}, {}, {}\n    h[-1] = h_prev\n    loss = 0\n\n    # FORWARD PASS\n    for t in range(seq_len):\n        X[t] = one_hot_encode([x_seq[t]], vocab_size)  # Shape: (vocab_size, 1)\n        h[t] = np.tanh(np.dot(Wxh, X[t]) + np.dot(Whh, h[t - 1]) + bxh)\n        y_pred[t] = np.dot(Why, h[t]) + bhy\n        p[t] = np.exp(y_pred[t]) / np.sum(np.exp(y_pred[t]))  # softmax\n        loss += -np.log(p[t][y_seq[t], 0])\n\n    # BACKWARD PASS\n    dWxh = np.zeros_like(Wxh)\n    dWhh = np.zeros_like(Whh)\n    dWhy = np.zeros_like(Why)\n    dbxh = np.zeros_like(bxh)\n    dbhy = np.zeros_like(bhy)\n    dh_next = np.zeros_like(h[0])\n\n    for t in reversed(range(seq_len)):\n        dy = p[t].copy()\n        dy[y_seq[t]] -= 1  # Gradient of cross-entropy loss\n        dWhy += np.dot(dy, h[t].T)\n        dbhy += dy\n\n        dh = np.dot(Why.T, dy) + dh_next\n        dtanh = (1 - h[t] ** 2) * dh  # Derivative of tanh\n        dWxh += np.dot(dtanh, X[t].T)\n        dWhh += np.dot(dtanh, h[t - 1].T)\n        dbxh += dtanh\n        dh_next = np.dot(Whh.T, dtanh)\n\n    # Optional: Gradient clipping\n    for dparam in [dWxh, dWhh, dWhy, dbxh, dbhy]:\n        np.clip(dparam, -5, 5, out=dparam)\n\n    return {\n        'dWxh': dWxh,\n        'dWhh': dWhh,\n        'dWhy': dWhy,\n        'dbxh': dbxh,\n        'dbhy': dbhy,\n        'loss': loss,\n        'h_last': h[seq_len - 1]\n    }\n\n\ndef update_params(params, grads, learning_rate):\n    \"\"\"\n    Update RNN parameters using SGD.\n    \n    Args:\n        params (dict): Model parameters.\n        grads (dict): Corresponding gradients.\n        learning_rate (float): Learning rate.\n    \"\"\"\n    for param in params:\n        # Clip gradients to prevent exploding gradients\n        np.clip(grads[param], -5, 5, out=grads[param])\n        # Update parameters using SGD\n        params[param] -= learning_rate * grads[param]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.019583Z","iopub.execute_input":"2025-06-21T06:01:33.019923Z","iopub.status.idle":"2025-06-21T06:01:33.036023Z","shell.execute_reply.started":"2025-06-21T06:01:33.019894Z","shell.execute_reply":"2025-06-21T06:01:33.035453Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ------------------------\n# Activation Functions\n# ------------------------\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation.\"\"\"\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    \"\"\"Derivative of tanh for backpropagation.\"\"\"\n    return 1.0 - np.tanh(x) ** 2\n\ndef softmax(x):\n    \"\"\"Numerically stable softmax.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x)\n\ndef softmax_with_temperature(x, temperature=1.0):\n    \"\"\"Softmax with temperature scaling.\"\"\"\n    scaled_x = x / temperature\n    e_x = np.exp(scaled_x - np.max(scaled_x))  # numerical stability\n    return e_x / np.sum(e_x)\n\n# ------------------------\n# One-Hot Encoding Utility\n# ------------------------\n\ndef one_hot_encode(indices, vocab_size):\n    \"\"\"\n    Convert list of indices to one-hot encoded 2D array.\n    Each column represents one-hot encoding of a character.\n    \"\"\"\n    result = np.zeros((vocab_size, len(indices)))\n    for i, idx in enumerate(indices):\n        result[idx, i] = 1\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.036854Z","iopub.execute_input":"2025-06-21T06:01:33.037161Z","iopub.status.idle":"2025-06-21T06:01:33.062061Z","shell.execute_reply.started":"2025-06-21T06:01:33.037133Z","shell.execute_reply":"2025-06-21T06:01:33.061146Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def initialize_rnn(hidden_size, vocab_size, seed=42):\n    \"\"\"\n    Initialize RNN model parameters and training hyperparameters.\n\n    Args:\n        hidden_size (int): Number of neurons in the hidden layer.\n        vocab_size (int): Size of the vocabulary (input and output dimension).\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        params (dict): Dictionary of RNN weights and biases.\n        hypers (dict): Dictionary of training hyperparameters.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Xavier-like initialization with small random values\n    params = {\n        'Wxh': np.random.randn(hidden_size, vocab_size) * 0.01,       # Input-to-hidden weights\n        'Whh': np.random.randn(hidden_size, hidden_size) * 0.01,      # Hidden-to-hidden weights\n        'Why': np.random.randn(vocab_size, hidden_size) * 0.01,       # Hidden-to-output weights\n        'bxh': np.zeros((hidden_size, 1)),                            # Hidden layer bias\n        'bhy': np.zeros((vocab_size, 1)),                             # Output layer bias\n    }\n\n    # Training hyperparameters\n    hypers = {\n        'alpha': 5e-3,     # Learning rate (was tuned earlier)\n        'seq_length': 50,  # Number of time steps per sequence\n        'steps': 10000,    # Total number of training steps\n        'grad_clip': 5.0   # Gradient clipping threshold (optional to apply)\n    }\n\n    return params, hypers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:06.584264Z","iopub.execute_input":"2025-06-21T06:24:06.584540Z","iopub.status.idle":"2025-06-21T06:24:06.590635Z","shell.execute_reply.started":"2025-06-21T06:24:06.584516Z","shell.execute_reply":"2025-06-21T06:24:06.589796Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ===========================\n# üì¶ DATA PREP & BATCHING MODULE\n# Contains:\n# - get_vocab_mappings()\n# - encode_data()\n# - create_training_sequences()\n# - batch_generator()\n# ===========================\n\nimport numpy as np\n\ndef get_vocab_mappings(data):\n    \"\"\"\n    Builds vocabulary and mapping dictionaries from input text data.\n    \n    Args:\n        data (str): Entire text corpus\n    \n    Returns:\n        Tuple[dict, dict, int]: char_to_ix, ix_to_char, vocab_size\n    \"\"\"\n    chars = sorted(list(set(data)))\n    vocab_size = len(chars)\n    char_to_ix = {ch: i for i, ch in enumerate(chars)}\n    ix_to_char = {i: ch for ch, i in char_to_ix.items()}\n    return char_to_ix, ix_to_char, vocab_size\n\ndef encode_data(data, char_to_ix):\n    \"\"\"\n    Encodes text data into a list of integer indices.\n    \n    Args:\n        data (str): Raw text data\n        char_to_ix (dict): Character-to-index mapping\n    \n    Returns:\n        List[int]: Encoded text as indices\n    \"\"\"\n    return [char_to_ix[ch] for ch in data]\n\ndef create_training_sequences(data_ix, seq_length=50, step=1):\n    \"\"\"\n    Splits encoded data into overlapping sequences of length `seq_length`.\n    \n    Args:\n        data_ix (List[int]): Encoded data indices\n        seq_length (int): Length of each input sequence\n        step (int): Stride between sequences\n    \n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Input and target sequences\n    \"\"\"\n    X = []\n    Y = []\n    for i in range(0, len(data_ix) - seq_length, step):\n        X.append(data_ix[i : i + seq_length])\n        Y.append(data_ix[i + 1 : i + seq_length + 1])\n    return np.array(X), np.array(Y)\n\ndef batch_generator(X_data, Y_data, batch_size):\n    \"\"\"\n    Yields mini-batches of input-target pairs.\n    \n    Args:\n        X_data (np.ndarray): Input sequences\n        Y_data (np.ndarray): Target sequences\n        batch_size (int): Number of sequences per batch\n    \n    Yields:\n        Tuple[np.ndarray, np.ndarray]: Mini-batches\n    \"\"\"\n    total_samples = len(X_data)\n    for i in range(0, total_samples, batch_size):\n        X_batch = X_data[i:i + batch_size]\n        Y_batch = Y_data[i:i + batch_size]\n        yield X_batch, Y_batch\n\n# ‚úÖ Example usage:\n# char_to_ix, ix_to_char, vocab_size = get_vocab_mappings(data)\n# data_ix = encode_data(data, char_to_ix)\n# X_data, Y_data = create_training_sequences(data_ix, seq_length=50)\n# for X_batch, Y_batch in batch_generator(X_data, Y_data, batch_size=64):\n#     ...  # pass to training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.081036Z","iopub.execute_input":"2025-06-21T06:01:33.081412Z","iopub.status.idle":"2025-06-21T06:01:33.110006Z","shell.execute_reply.started":"2025-06-21T06:01:33.081378Z","shell.execute_reply":"2025-06-21T06:01:33.109034Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ===========================\n# üöÇ TRAINING + SAMPLING LOOP\n# Contains:\n# - train_rnn(): main training loop\n# - sample_rnn(): generate text using trained model\n# ===========================\n\nimport time\n\ndef train_rnn(X_data, Y_data, params, hypers, char_to_ix, ix_to_char, print_every=100, start_text='A'):\n    \"\"\"\n    Trains the RNN on input data using SGD.\n    \n    Args:\n        data_ix (List[int]): Encoded character indices from text\n        params (dict): Model parameters\n        hypers (dict): Hyperparameters\n        char_to_ix (dict): Character to index mapping\n        ix_to_char (dict): Index to character mapping\n        print_every (int): Print loss and sample text every `print_every` steps\n        start_text (str): Seed text for sampling during training\n    \"\"\"\n    vocab_size = len(char_to_ix)\n    seq_length = hypers['seq_length']\n    learning_rate = hypers['alpha']\n    steps = hypers['steps']\n    grad_clip = hypers['grad_clip']\n    h_prev = np.zeros((params['Whh'].shape[0], 1))\n    start_time = time.time()\n    \n    \n    for step in range(steps):\n        i = step % len(X_data)\n        x_seq = X_data[i]\n        y_seq = Y_data[i]\n        \n        result = rnn_forward_backward(x_seq, y_seq, h_prev, params, vocab_size)\n        h_prev = result['h_last']\n        grads = {k: result['d' + k] for k in ['Wxh', 'Whh', 'Why', 'bxh', 'bhy']}\n        update_params(params, grads, learning_rate)\n        \n        if step % print_every == 0:\n            elapsed = (time.time() - start_time) / 60\n            print(f\"Step {step} | Loss: {result['loss']:.4f} | Time elapsed: {elapsed:.2f} min\")\n            #sample_text = sample_rnn(char_to_ix[start_text], h_prev, params, ix_to_char, n=300)\n            #print(\"---- Sample ----\")\n            #print(sample_text)\n            #print(\"----------------\\n\")\n\ndef sample_rnn(seed_ix, h_prev, params, ix_to_char, n=200, temperature=1.0):\n    \"\"\"\n    Generates a text sequence using the trained RNN.\n    \n    Args:\n        seed_ix (int): Index of the starting character\n        h_prev (np.ndarray): Initial hidden state\n        params (dict): RNN model parameters\n        ix_to_char (dict): Mapping from index to character\n        n (int): Number of characters to sample\n        temperature (float): Sampling temperature\n    \n    Returns:\n        str: Generated text\n    \"\"\"\n    Wxh, Whh, Why = params['Wxh'], params['Whh'], params['Why']\n    bxh, bhy = params['bxh'], params['bhy']\n    \n    vocab_size = Why.shape[0]\n    h = h_prev\n    x = np.zeros((vocab_size, 1))\n    x[seed_ix] = 1\n\n    generated = []\n\n    for _ in range(n):\n        h = tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bxh)\n        y = np.dot(Why, h) + bhy\n        p = softmax_with_temperature(y, temperature)\n        idx = np.random.choice(range(vocab_size), p=p.ravel())\n\n        x = np.zeros((vocab_size, 1))\n        x[idx] = 1\n        generated.append(ix_to_char[idx])\n\n    return ''.join(generated)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:22:33.149809Z","iopub.execute_input":"2025-06-21T06:22:33.150161Z","iopub.status.idle":"2025-06-21T06:22:33.160710Z","shell.execute_reply.started":"2025-06-21T06:22:33.150135Z","shell.execute_reply":"2025-06-21T06:22:33.160059Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Step 1: Load and preprocess text\nimport numpy as np\n\n# Load cleaned Asimov corpus\nwith open(\"/kaggle/input/asimov/asimov_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n    data = f.read()\n\n# Get all unique characters in the text\nchars = sorted(list(set(data)))\nvocab_size = len(chars)\n\n# Create character-to-index and index-to-character mappings\nchar_to_ix = {ch: i for i, ch in enumerate(chars)}\nix_to_char = {i: ch for ch, i in char_to_ix.items()}\n\n# Encode the entire text as a list of character indices\ndata_ix = [char_to_ix[ch] for ch in data]\n\n# Print some basic stats\nprint(f\"Total characters: {len(data)}\")\nprint(f\"Unique characters: {vocab_size}\")\nprint(f\"Sample char_to_ix: {list(char_to_ix.items())[:10]}\")\nprint(f\"Encoded text (first 20 indices): {data_ix[:20]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.138820Z","iopub.execute_input":"2025-06-21T06:01:33.139081Z","iopub.status.idle":"2025-06-21T06:01:33.899788Z","shell.execute_reply.started":"2025-06-21T06:01:33.139057Z","shell.execute_reply":"2025-06-21T06:01:33.898849Z"}},"outputs":[{"name":"stdout","text":"Total characters: 10925424\nUnique characters: 105\nSample char_to_ix: [(' ', 0), ('!', 1), ('\"', 2), ('#', 3), ('$', 4), ('%', 5), ('&', 6), (\"'\", 7), ('(', 8), (')', 9)]\nEncoded text (first 20 indices): [31, 72, 80, 68, 75, 81, 67, 68, 0, 39, 0, 68, 61, 82, 65, 0, 83, 78, 69, 80]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Preprocessing\nchar_to_ix, ix_to_char, vocab_size = get_vocab_mappings(data)\ndata_ix = encode_data(data, char_to_ix)\n\n# Create training sequences (do this once and reuse)\nX_data, Y_data = create_training_sequences(data_ix, seq_length=hypers['seq_length'])\n\n# Optionally save to disk\nnp.save(\"X_data.npy\", X_data)\nnp.save(\"Y_data.npy\", Y_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:09:57.692822Z","iopub.execute_input":"2025-06-21T06:09:57.693238Z","iopub.status.idle":"2025-06-21T06:12:25.038732Z","shell.execute_reply.started":"2025-06-21T06:09:57.693210Z","shell.execute_reply":"2025-06-21T06:12:25.037677Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load training data\nX_data = np.load(\"X_data.npy\")\nY_data = np.load(\"Y_data.npy\")\n\nprint(\"Loaded training data\")\n\n# Initialize model\nparams, hypers = initialize_rnn(hidden_size=128, vocab_size=vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:21:48.789571Z","iopub.status.idle":"2025-06-21T06:21:48.790059Z","shell.execute_reply.started":"2025-06-21T06:21:48.789851Z","shell.execute_reply":"2025-06-21T06:21:48.789871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\ntrained_params, final_hidden = train_rnn(\n    X_data,\n    Y_data,\n    params,\n    hypers,\n    char_to_ix,\n    ix_to_char,\n    print_every=100,\n    start_text=\"T\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:22:43.070475Z","iopub.execute_input":"2025-06-21T06:22:43.070758Z","iopub.status.idle":"2025-06-21T06:23:54.959307Z","shell.execute_reply.started":"2025-06-21T06:22:43.070735Z","shell.execute_reply":"2025-06-21T06:23:54.957951Z"}},"outputs":[{"name":"stdout","text":"Step 0 | Loss: 178.1395 | Time elapsed: 0.00 min\nStep 100 | Loss: 97.3233 | Time elapsed: 0.01 min\nStep 200 | Loss: 81.9243 | Time elapsed: 0.02 min\nStep 300 | Loss: 77.0342 | Time elapsed: 0.04 min\nStep 400 | Loss: 107.4061 | Time elapsed: 0.05 min\nStep 500 | Loss: 95.6206 | Time elapsed: 0.06 min\nStep 600 | Loss: 92.0886 | Time elapsed: 0.07 min\nStep 700 | Loss: 79.1421 | Time elapsed: 0.08 min\nStep 800 | Loss: 103.0429 | Time elapsed: 0.10 min\nStep 900 | Loss: 82.4444 | Time elapsed: 0.11 min\nStep 1000 | Loss: 64.2677 | Time elapsed: 0.12 min\nStep 1100 | Loss: 117.7956 | Time elapsed: 0.13 min\nStep 1200 | Loss: 111.3303 | Time elapsed: 0.15 min\nStep 1300 | Loss: 225.4323 | Time elapsed: 0.16 min\nStep 1400 | Loss: 138.0527 | Time elapsed: 0.17 min\nStep 1500 | Loss: 125.6657 | Time elapsed: 0.18 min\nStep 1600 | Loss: 132.5989 | Time elapsed: 0.19 min\nStep 1700 | Loss: 119.0338 | Time elapsed: 0.21 min\nStep 1800 | Loss: 110.1690 | Time elapsed: 0.22 min\nStep 1900 | Loss: 124.4022 | Time elapsed: 0.23 min\nStep 2000 | Loss: 95.7285 | Time elapsed: 0.24 min\nStep 2100 | Loss: 100.6872 | Time elapsed: 0.25 min\nStep 2200 | Loss: 59.1687 | Time elapsed: 0.27 min\nStep 2300 | Loss: 60.0727 | Time elapsed: 0.28 min\nStep 2400 | Loss: 110.6582 | Time elapsed: 0.29 min\nStep 2500 | Loss: 71.5170 | Time elapsed: 0.30 min\nStep 2600 | Loss: 46.3410 | Time elapsed: 0.31 min\nStep 2700 | Loss: 67.1915 | Time elapsed: 0.33 min\nStep 2800 | Loss: 59.3074 | Time elapsed: 0.34 min\nStep 2900 | Loss: 52.2212 | Time elapsed: 0.35 min\nStep 3000 | Loss: 59.8868 | Time elapsed: 0.36 min\nStep 3100 | Loss: 98.1646 | Time elapsed: 0.37 min\nStep 3200 | Loss: 40.8956 | Time elapsed: 0.39 min\nStep 3300 | Loss: 74.2716 | Time elapsed: 0.40 min\nStep 3400 | Loss: 71.2618 | Time elapsed: 0.41 min\nStep 3500 | Loss: 78.4867 | Time elapsed: 0.42 min\nStep 3600 | Loss: 96.4314 | Time elapsed: 0.43 min\nStep 3700 | Loss: 77.4442 | Time elapsed: 0.45 min\nStep 3800 | Loss: 91.6467 | Time elapsed: 0.46 min\nStep 3900 | Loss: 45.1045 | Time elapsed: 0.47 min\nStep 4000 | Loss: 80.8378 | Time elapsed: 0.48 min\nStep 4100 | Loss: 59.9347 | Time elapsed: 0.49 min\nStep 4200 | Loss: 192.1580 | Time elapsed: 0.50 min\nStep 4300 | Loss: 158.1037 | Time elapsed: 0.52 min\nStep 4400 | Loss: 136.1591 | Time elapsed: 0.53 min\nStep 4500 | Loss: 126.8372 | Time elapsed: 0.54 min\nStep 4600 | Loss: 177.6792 | Time elapsed: 0.55 min\nStep 4700 | Loss: 116.9439 | Time elapsed: 0.56 min\nStep 4800 | Loss: 138.1030 | Time elapsed: 0.58 min\nStep 4900 | Loss: 175.9598 | Time elapsed: 0.59 min\nStep 5000 | Loss: 147.0903 | Time elapsed: 0.60 min\nStep 5100 | Loss: 132.9898 | Time elapsed: 0.61 min\nStep 5200 | Loss: 111.3382 | Time elapsed: 0.62 min\nStep 5300 | Loss: 143.9220 | Time elapsed: 0.64 min\nStep 5400 | Loss: 114.5406 | Time elapsed: 0.65 min\nStep 5500 | Loss: 128.3323 | Time elapsed: 0.66 min\nStep 5600 | Loss: 118.1277 | Time elapsed: 0.67 min\nStep 5700 | Loss: 135.3428 | Time elapsed: 0.69 min\nStep 5800 | Loss: 123.1467 | Time elapsed: 0.70 min\nStep 5900 | Loss: 129.1224 | Time elapsed: 0.71 min\nStep 6000 | Loss: 118.5135 | Time elapsed: 0.72 min\nStep 6100 | Loss: 150.3981 | Time elapsed: 0.73 min\nStep 6200 | Loss: 98.4859 | Time elapsed: 0.75 min\nStep 6300 | Loss: 148.9058 | Time elapsed: 0.76 min\nStep 6400 | Loss: 124.4532 | Time elapsed: 0.77 min\nStep 6500 | Loss: 127.7060 | Time elapsed: 0.78 min\nStep 6600 | Loss: 119.1446 | Time elapsed: 0.79 min\nStep 6700 | Loss: 127.8008 | Time elapsed: 0.81 min\nStep 6800 | Loss: 125.2241 | Time elapsed: 0.82 min\nStep 6900 | Loss: 110.3043 | Time elapsed: 0.83 min\nStep 7000 | Loss: 117.1911 | Time elapsed: 0.84 min\nStep 7100 | Loss: 118.7709 | Time elapsed: 0.85 min\nStep 7200 | Loss: 123.0437 | Time elapsed: 0.86 min\nStep 7300 | Loss: 127.4883 | Time elapsed: 0.88 min\nStep 7400 | Loss: 125.2837 | Time elapsed: 0.89 min\nStep 7500 | Loss: 122.1702 | Time elapsed: 0.90 min\nStep 7600 | Loss: 124.6583 | Time elapsed: 0.91 min\nStep 7700 | Loss: 125.7887 | Time elapsed: 0.92 min\nStep 7800 | Loss: 115.3325 | Time elapsed: 0.94 min\nStep 7900 | Loss: 120.4990 | Time elapsed: 0.95 min\nStep 8000 | Loss: 102.3857 | Time elapsed: 0.96 min\nStep 8100 | Loss: 135.8118 | Time elapsed: 0.97 min\nStep 8200 | Loss: 167.8164 | Time elapsed: 0.98 min\nStep 8300 | Loss: 111.1914 | Time elapsed: 1.00 min\nStep 8400 | Loss: 119.6141 | Time elapsed: 1.01 min\nStep 8500 | Loss: 120.6624 | Time elapsed: 1.02 min\nStep 8600 | Loss: 108.3149 | Time elapsed: 1.03 min\nStep 8700 | Loss: 116.7619 | Time elapsed: 1.04 min\nStep 8800 | Loss: 114.5228 | Time elapsed: 1.05 min\nStep 8900 | Loss: 141.3764 | Time elapsed: 1.07 min\nStep 9000 | Loss: 119.3643 | Time elapsed: 1.08 min\nStep 9100 | Loss: 106.9463 | Time elapsed: 1.09 min\nStep 9200 | Loss: 111.4826 | Time elapsed: 1.10 min\nStep 9300 | Loss: 118.2281 | Time elapsed: 1.11 min\nStep 9400 | Loss: 116.4731 | Time elapsed: 1.13 min\nStep 9500 | Loss: 137.8623 | Time elapsed: 1.14 min\nStep 9600 | Loss: 94.5807 | Time elapsed: 1.15 min\nStep 9700 | Loss: 108.2947 | Time elapsed: 1.16 min\nStep 9800 | Loss: 101.6883 | Time elapsed: 1.17 min\nStep 9900 | Loss: 107.8570 | Time elapsed: 1.19 min\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4134412677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trained_params, final_hidden = train_rnn(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mY_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"],"ename":"TypeError","evalue":"cannot unpack non-iterable NoneType object","output_type":"error"}],"execution_count":11}]}