{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12201425,"sourceType":"datasetVersion","datasetId":7685774}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Character-Level RNN: Text Generation from Asimov's *Foundation*\n\nğŸ§  Goal:\nBuild a character-level RNN from scratch using NumPy to model language patterns in Isaac Asimov's *Foundation*.\nThe trained model will generate Asimov-style text one character at a time.\n\nğŸ” Plan:\n\n1. ğŸ“– Load and preprocess text\n   - Read *Foundation* text\n   - Create character-to-index (char2idx) and index-to-character (idx2char) mappings\n   - Encode text as integer sequence\n\n2. ğŸ§± Initialize RNN model\n   - Parameters: Wxh, Whh, Why, bh, by\n   - Hidden state size: e.g., 100\n\n3. ğŸ”„ Forward and Backward Pass\n   - Implement one time-step forward: h_t = tanh(WxhÂ·x_t + WhhÂ·h_{t-1} + bh)\n   - Predict next char logits: y_t = WhyÂ·h_t + by\n   - Use softmax + cross-entropy loss\n   - Backpropagate gradients through time (BPTT)\n   - Apply gradient clipping\n\n4. ğŸ‹ï¸â€â™‚ï¸ Training Loop\n   - Slide a window over text with fixed sequence length (e.g., 25 chars)\n   - Compute loss and gradients\n   - Update parameters via SGD\n\n5. ğŸ§ª Sampling Function\n   - Start from a seed character\n   - Sample next character from softmax distribution\n   - Repeat for N characters\n\n6. ğŸ“‰ Monitoring\n   - Print loss every N steps\n   - Print sample text every 1000 iterations\n\n7. ğŸš€ [Optional] Try with different corpora (e.g., Sanskrit, Shakespeare)\n\nThis notebook is a stepping stone toward building a Sanskrit name generator using character-level RNNs.\n","metadata":{}},{"cell_type":"code","source":"# Step 1: Load and preprocess text\nimport numpy as np\n\n# Load cleaned Asimov corpus\nwith open(\"/kaggle/input/asimov/asimov_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n    data = f.read()\n\n# Get all unique characters in the text\nchars = sorted(list(set(data)))\nvocab_size = len(chars)\n\n# Create character-to-index and index-to-character mappings\nchar_to_ix = {ch: i for i, ch in enumerate(chars)}\nix_to_char = {i: ch for ch, i in char_to_ix.items()}\n\n# Encode the entire text as a list of character indices\ndata_ix = [char_to_ix[ch] for ch in data]\n\n# Print some basic stats\nprint(f\"Total characters: {len(data)}\")\nprint(f\"Unique characters: {vocab_size}\")\nprint(f\"Sample char_to_ix: {list(char_to_ix.items())[:10]}\")\nprint(f\"Encoded text (first 20 indices): {data_ix[:20]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T03:26:41.301478Z","iopub.execute_input":"2025-06-18T03:26:41.301695Z","iopub.status.idle":"2025-06-18T03:26:42.190770Z","shell.execute_reply.started":"2025-06-18T03:26:41.301674Z","shell.execute_reply":"2025-06-18T03:26:42.189828Z"}},"outputs":[{"name":"stdout","text":"Total characters: 10925424\nUnique characters: 105\nSample char_to_ix: [(' ', 0), ('!', 1), ('\"', 2), ('#', 3), ('$', 4), ('%', 5), ('&', 6), (\"'\", 7), ('(', 8), (')', 9)]\nEncoded text (first 20 indices): [31, 72, 80, 68, 75, 81, 67, 68, 0, 39, 0, 68, 61, 82, 65, 0, 83, 78, 69, 80]\n","output_type":"stream"}],"execution_count":1}]}