{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12201425,"sourceType":"datasetVersion","datasetId":7685774}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Character-Level RNN: Text Generation from Asimov's *Foundation*\n\nüß† Goal:\nBuild a character-level RNN from scratch using NumPy to model language patterns in Isaac Asimov's *Foundation*.\nThe trained model will generate Asimov-style text one character at a time.\n\nüîÅ Plan:\n\n1. üìñ Load and preprocess text\n   - Read *Foundation* text\n   - Create character-to-index (char2idx) and index-to-character (idx2char) mappings\n   - Encode text as integer sequence\n\n2. üß± Initialize RNN model\n   - Parameters: Wxh, Whh, Why, bh, by\n   - Hidden state size: e.g., 100\n\n3. üîÑ Forward and Backward Pass\n   - Implement one time-step forward: h_t = tanh(Wxh¬∑x_t + Whh¬∑h_{t-1} + bh)\n   - Predict next char logits: y_t = Why¬∑h_t + by\n   - Use softmax + cross-entropy loss\n   - Backpropagate gradients through time (BPTT)\n   - Apply gradient clipping\n\n4. üèãÔ∏è‚Äç‚ôÇÔ∏è Training Loop\n   - Slide a window over text with fixed sequence length (e.g., 25 chars)\n   - Compute loss and gradients\n   - Update parameters via SGD\n\n5. üß™ Sampling Function\n   - Start from a seed character\n   - Sample next character from softmax distribution\n   - Repeat for N characters\n\n6. üìâ Monitoring\n   - Print loss every N steps\n   - Print sample text every 1000 iterations\n\n7. üöÄ [Optional] Try with different corpora (e.g., Sanskrit, Shakespeare)\n\nThis notebook is a stepping stone toward building a Sanskrit name generator using character-level RNNs.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.012839Z","iopub.execute_input":"2025-06-21T06:01:33.013173Z","iopub.status.idle":"2025-06-21T06:01:33.018070Z","shell.execute_reply.started":"2025-06-21T06:01:33.013146Z","shell.execute_reply":"2025-06-21T06:01:33.017077Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ===========================\n# üîÅ RNN FORWARD + BACKWARD MODULE\n# Contains:\n# - rnn_forward_backward(): forward and backward pass of a simple RNN\n# - update_params(): SGD update with optional gradient clipping\n# ===========================\n\n\ndef rnn_forward_backward(x_seq, y_seq, h_prev, params, vocab_size):\n    \"\"\"\n    Perform a forward and backward pass of a vanilla RNN.\n    \n    Args:\n        x_seq (list[int]): Sequence of input character indices.\n        y_seq (list[int]): Sequence of target character indices.\n        h_prev (np.ndarray): Hidden state from previous batch.\n        params (dict): Dictionary of model parameters.\n        vocab_size (int): Size of vocabulary.\n        \n    Returns:\n        dict: Gradients for parameters, final hidden state, and total loss.\n    \"\"\"\n    Wxh, Whh, Why = params['Wxh'], params['Whh'], params['Why']\n    bxh, bhy = params['bxh'], params['bhy']\n\n    seq_len = len(x_seq)\n    h, y_pred, p, X = {}, {}, {}, {}\n    h[-1] = h_prev\n    loss = 0\n\n    # FORWARD PASS\n    for t in range(seq_len):\n        X[t] = one_hot_encode([x_seq[t]], vocab_size)  # Shape: (vocab_size, 1)\n        h[t] = np.tanh(np.dot(Wxh, X[t]) + np.dot(Whh, h[t - 1]) + bxh)\n        y_pred[t] = np.dot(Why, h[t]) + bhy\n        p[t] = np.exp(y_pred[t]) / np.sum(np.exp(y_pred[t]))  # softmax\n        loss += -np.log(p[t][y_seq[t], 0])\n\n    # BACKWARD PASS\n    dWxh = np.zeros_like(Wxh)\n    dWhh = np.zeros_like(Whh)\n    dWhy = np.zeros_like(Why)\n    dbxh = np.zeros_like(bxh)\n    dbhy = np.zeros_like(bhy)\n    dh_next = np.zeros_like(h[0])\n\n    for t in reversed(range(seq_len)):\n        dy = p[t].copy()\n        dy[y_seq[t]] -= 1  # Gradient of cross-entropy loss\n        dWhy += np.dot(dy, h[t].T)\n        dbhy += dy\n\n        dh = np.dot(Why.T, dy) + dh_next\n        dtanh = (1 - h[t] ** 2) * dh  # Derivative of tanh\n        dWxh += np.dot(dtanh, X[t].T)\n        dWhh += np.dot(dtanh, h[t - 1].T)\n        dbxh += dtanh\n        dh_next = np.dot(Whh.T, dtanh)\n\n    # Optional: Gradient clipping\n    #for dparam in [dWxh, dWhh, dWhy, dbxh, dbhy]:\n        #np.clip(dparam, -5, 5, out=dparam)\n\n    return {\n        'dWxh': dWxh,\n        'dWhh': dWhh,\n        'dWhy': dWhy,\n        'dbxh': dbxh,\n        'dbhy': dbhy,\n        'loss': loss,\n        'h_last': h[seq_len - 1]\n    }\n\n\ndef update_params(params, grads, learning_rate):\n    \"\"\"\n    Update RNN parameters using SGD.\n    \n    Args:\n        params (dict): Model parameters.\n        grads (dict): Corresponding gradients.\n        learning_rate (float): Learning rate.\n    \"\"\"\n    for param in params:\n        # Clip gradients to prevent exploding gradients\n        #np.clip(grads[param], -5, 5, out=grads[param])\n        # Update parameters using SGD\n        params[param] -= learning_rate * grads[param]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:36:35.465661Z","iopub.execute_input":"2025-06-21T06:36:35.466278Z","iopub.status.idle":"2025-06-21T06:36:35.475918Z","shell.execute_reply.started":"2025-06-21T06:36:35.466254Z","shell.execute_reply":"2025-06-21T06:36:35.475175Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ------------------------\n# Activation Functions\n# ------------------------\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation.\"\"\"\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    \"\"\"Derivative of tanh for backpropagation.\"\"\"\n    return 1.0 - np.tanh(x) ** 2\n\ndef softmax(x):\n    \"\"\"Numerically stable softmax.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x)\n\ndef softmax_with_temperature(x, temperature=1.0):\n    \"\"\"Softmax with temperature scaling.\"\"\"\n    scaled_x = x / temperature\n    e_x = np.exp(scaled_x - np.max(scaled_x))  # numerical stability\n    return e_x / np.sum(e_x)\n\n# ------------------------\n# One-Hot Encoding Utility\n# ------------------------\n\ndef one_hot_encode(indices, vocab_size):\n    \"\"\"\n    Convert list of indices to one-hot encoded 2D array.\n    Each column represents one-hot encoding of a character.\n    \"\"\"\n    result = np.zeros((vocab_size, len(indices)))\n    for i, idx in enumerate(indices):\n        result[idx, i] = 1\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.036854Z","iopub.execute_input":"2025-06-21T06:01:33.037161Z","iopub.status.idle":"2025-06-21T06:01:33.062061Z","shell.execute_reply.started":"2025-06-21T06:01:33.037133Z","shell.execute_reply":"2025-06-21T06:01:33.061146Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def initialize_rnn(hidden_size, vocab_size, seed=42):\n    \"\"\"\n    Initialize RNN model parameters and training hyperparameters.\n\n    Args:\n        hidden_size (int): Number of neurons in the hidden layer.\n        vocab_size (int): Size of the vocabulary (input and output dimension).\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        params (dict): Dictionary of RNN weights and biases.\n        hypers (dict): Dictionary of training hyperparameters.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Xavier-like initialization with small random values\n    params = {\n        'Wxh': np.random.randn(hidden_size, vocab_size) * 0.01,       # Input-to-hidden weights\n        'Whh': np.random.randn(hidden_size, hidden_size) * 0.01,      # Hidden-to-hidden weights\n        'Why': np.random.randn(vocab_size, hidden_size) * 0.01,       # Hidden-to-output weights\n        'bxh': np.zeros((hidden_size, 1)),                            # Hidden layer bias\n        'bhy': np.zeros((vocab_size, 1)),                             # Output layer bias\n    }\n\n    # Training hyperparameters\n    hypers = {\n        'alpha': 5e-3,     # Learning rate (was tuned earlier)\n        'seq_length': 50,  # Number of time steps per sequence\n        'steps': 10000,    # Total number of training steps\n        'grad_clip': 5.0   # Gradient clipping threshold (optional to apply)\n    }\n\n    return params, hypers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:24:06.584264Z","iopub.execute_input":"2025-06-21T06:24:06.584540Z","iopub.status.idle":"2025-06-21T06:24:06.590635Z","shell.execute_reply.started":"2025-06-21T06:24:06.584516Z","shell.execute_reply":"2025-06-21T06:24:06.589796Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ===========================\n# üì¶ DATA PREP & BATCHING MODULE\n# Contains:\n# - get_vocab_mappings()\n# - encode_data()\n# - create_training_sequences()\n# - batch_generator()\n# ===========================\n\nimport numpy as np\n\ndef get_vocab_mappings(data):\n    \"\"\"\n    Builds vocabulary and mapping dictionaries from input text data.\n    \n    Args:\n        data (str): Entire text corpus\n    \n    Returns:\n        Tuple[dict, dict, int]: char_to_ix, ix_to_char, vocab_size\n    \"\"\"\n    chars = sorted(list(set(data)))\n    vocab_size = len(chars)\n    char_to_ix = {ch: i for i, ch in enumerate(chars)}\n    ix_to_char = {i: ch for ch, i in char_to_ix.items()}\n    return char_to_ix, ix_to_char, vocab_size\n\ndef encode_data(data, char_to_ix):\n    \"\"\"\n    Encodes text data into a list of integer indices.\n    \n    Args:\n        data (str): Raw text data\n        char_to_ix (dict): Character-to-index mapping\n    \n    Returns:\n        List[int]: Encoded text as indices\n    \"\"\"\n    return [char_to_ix[ch] for ch in data]\n\ndef create_training_sequences(data_ix, seq_length=50, step=1):\n    \"\"\"\n    Splits encoded data into overlapping sequences of length `seq_length`.\n    \n    Args:\n        data_ix (List[int]): Encoded data indices\n        seq_length (int): Length of each input sequence\n        step (int): Stride between sequences\n    \n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Input and target sequences\n    \"\"\"\n    X = []\n    Y = []\n    for i in range(0, len(data_ix) - seq_length, step):\n        X.append(data_ix[i : i + seq_length])\n        Y.append(data_ix[i + 1 : i + seq_length + 1])\n    return np.array(X), np.array(Y)\n\ndef batch_generator(X_data, Y_data, batch_size):\n    \"\"\"\n    Yields mini-batches of input-target pairs.\n    \n    Args:\n        X_data (np.ndarray): Input sequences\n        Y_data (np.ndarray): Target sequences\n        batch_size (int): Number of sequences per batch\n    \n    Yields:\n        Tuple[np.ndarray, np.ndarray]: Mini-batches\n    \"\"\"\n    total_samples = len(X_data)\n    for i in range(0, total_samples, batch_size):\n        X_batch = X_data[i:i + batch_size]\n        Y_batch = Y_data[i:i + batch_size]\n        yield X_batch, Y_batch\n\n# ‚úÖ Example usage:\n# char_to_ix, ix_to_char, vocab_size = get_vocab_mappings(data)\n# data_ix = encode_data(data, char_to_ix)\n# X_data, Y_data = create_training_sequences(data_ix, seq_length=50)\n# for X_batch, Y_batch in batch_generator(X_data, Y_data, batch_size=64):\n#     ...  # pass to training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.081036Z","iopub.execute_input":"2025-06-21T06:01:33.081412Z","iopub.status.idle":"2025-06-21T06:01:33.110006Z","shell.execute_reply.started":"2025-06-21T06:01:33.081378Z","shell.execute_reply":"2025-06-21T06:01:33.109034Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ===========================\n# üöÇ TRAINING + SAMPLING LOOP\n# Contains:\n# - train_rnn(): main training loop\n# - sample_rnn(): generate text using trained model\n# ===========================\n\nimport time\n\ndef train_rnn(X_data, Y_data, params, hypers, char_to_ix, ix_to_char, print_every=100, start_text='A'):\n    \"\"\"\n    Trains the RNN on input data using SGD.\n    \n    Args:\n        data_ix (List[int]): Encoded character indices from text\n        params (dict): Model parameters\n        hypers (dict): Hyperparameters\n        char_to_ix (dict): Character to index mapping\n        ix_to_char (dict): Index to character mapping\n        print_every (int): Print loss and sample text every `print_every` steps\n        start_text (str): Seed text for sampling during training\n    \"\"\"\n    vocab_size = len(char_to_ix)\n    seq_length = hypers['seq_length']\n    learning_rate = hypers['alpha']\n    steps = hypers['steps']\n    grad_clip = hypers['grad_clip']\n    h_prev = np.zeros((params['Whh'].shape[0], 1))\n    start_time = time.time()\n    \n    \n    for step in range(steps):\n        i = step % len(X_data)\n        x_seq = X_data[i]\n        y_seq = Y_data[i]\n        \n        result = rnn_forward_backward(x_seq, y_seq, h_prev, params, vocab_size)\n        h_prev = result['h_last']\n        grads = {k: result['d' + k] for k in ['Wxh', 'Whh', 'Why', 'bxh', 'bhy']}\n        update_params(params, grads, learning_rate)\n        \n        if step % print_every == 0:\n            elapsed = (time.time() - start_time) / 60\n            print(f\"Step {step} | Loss: {result['loss']:.4f} | Time elapsed: {elapsed:.2f} min\")\n            #sample_text = sample_rnn(char_to_ix[start_text], h_prev, params, ix_to_char, n=300)\n            #print(\"---- Sample ----\")\n            #print(sample_text)\n            #print(\"----------------\\n\")\n\ndef sample_rnn(seed_ix, h_prev, params, ix_to_char, n=200, temperature=1.0):\n    \"\"\"\n    Generates a text sequence using the trained RNN.\n    \n    Args:\n        seed_ix (int): Index of the starting character\n        h_prev (np.ndarray): Initial hidden state\n        params (dict): RNN model parameters\n        ix_to_char (dict): Mapping from index to character\n        n (int): Number of characters to sample\n        temperature (float): Sampling temperature\n    \n    Returns:\n        str: Generated text\n    \"\"\"\n    Wxh, Whh, Why = params['Wxh'], params['Whh'], params['Why']\n    bxh, bhy = params['bxh'], params['bhy']\n    \n    vocab_size = Why.shape[0]\n    h = h_prev\n    x = np.zeros((vocab_size, 1))\n    x[seed_ix] = 1\n\n    generated = []\n\n    for _ in range(n):\n        h = tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bxh)\n        y = np.dot(Why, h) + bhy\n        p = softmax_with_temperature(y, temperature)\n        idx = np.random.choice(range(vocab_size), p=p.ravel())\n\n        x = np.zeros((vocab_size, 1))\n        x[idx] = 1\n        generated.append(ix_to_char[idx])\n\n    return ''.join(generated)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:22:33.149809Z","iopub.execute_input":"2025-06-21T06:22:33.150161Z","iopub.status.idle":"2025-06-21T06:22:33.160710Z","shell.execute_reply.started":"2025-06-21T06:22:33.150135Z","shell.execute_reply":"2025-06-21T06:22:33.160059Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Step 1: Load and preprocess text\nimport numpy as np\n\n# Load cleaned Asimov corpus\nwith open(\"/kaggle/input/asimov/asimov_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n    data = f.read()\n\n# Get all unique characters in the text\nchars = sorted(list(set(data)))\nvocab_size = len(chars)\n\n# Create character-to-index and index-to-character mappings\nchar_to_ix = {ch: i for i, ch in enumerate(chars)}\nix_to_char = {i: ch for ch, i in char_to_ix.items()}\n\n# Encode the entire text as a list of character indices\ndata_ix = [char_to_ix[ch] for ch in data]\n\n# Print some basic stats\nprint(f\"Total characters: {len(data)}\")\nprint(f\"Unique characters: {vocab_size}\")\nprint(f\"Sample char_to_ix: {list(char_to_ix.items())[:10]}\")\nprint(f\"Encoded text (first 20 indices): {data_ix[:20]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:01:33.138820Z","iopub.execute_input":"2025-06-21T06:01:33.139081Z","iopub.status.idle":"2025-06-21T06:01:33.899788Z","shell.execute_reply.started":"2025-06-21T06:01:33.139057Z","shell.execute_reply":"2025-06-21T06:01:33.898849Z"}},"outputs":[{"name":"stdout","text":"Total characters: 10925424\nUnique characters: 105\nSample char_to_ix: [(' ', 0), ('!', 1), ('\"', 2), ('#', 3), ('$', 4), ('%', 5), ('&', 6), (\"'\", 7), ('(', 8), (')', 9)]\nEncoded text (first 20 indices): [31, 72, 80, 68, 75, 81, 67, 68, 0, 39, 0, 68, 61, 82, 65, 0, 83, 78, 69, 80]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Preprocessing\nchar_to_ix, ix_to_char, vocab_size = get_vocab_mappings(data)\ndata_ix = encode_data(data, char_to_ix)\n\n# Create training sequences (do this once and reuse)\nX_data, Y_data = create_training_sequences(data_ix, seq_length=hypers['seq_length'])\n\n# Optionally save to disk\nnp.save(\"X_data.npy\", X_data)\nnp.save(\"Y_data.npy\", Y_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:09:57.692822Z","iopub.execute_input":"2025-06-21T06:09:57.693238Z","iopub.status.idle":"2025-06-21T06:12:25.038732Z","shell.execute_reply.started":"2025-06-21T06:09:57.693210Z","shell.execute_reply":"2025-06-21T06:12:25.037677Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load training data\nX_data = np.load(\"X_data.npy\")\nY_data = np.load(\"Y_data.npy\")\n\nprint(\"Loaded training data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:21:48.789571Z","iopub.status.idle":"2025-06-21T06:21:48.790059Z","shell.execute_reply.started":"2025-06-21T06:21:48.789851Z","shell.execute_reply":"2025-06-21T06:21:48.789871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model\nparams, hypers = initialize_rnn(hidden_size=512, vocab_size=vocab_size)\n\n# Train the model\ntrained_params, final_hidden = train_rnn(\n    X_data,\n    Y_data,\n    params,\n    hypers,\n    char_to_ix,\n    ix_to_char,\n    print_every=100,\n    start_text=\"T\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:38:46.289109Z","iopub.execute_input":"2025-06-21T06:38:46.289650Z","iopub.status.idle":"2025-06-21T06:39:13.542761Z","shell.execute_reply.started":"2025-06-21T06:38:46.289625Z","shell.execute_reply":"2025-06-21T06:39:13.541716Z"}},"outputs":[{"name":"stdout","text":"Step 0 | Loss: 232.6956 | Time elapsed: 0.00 min\nStep 100 | Loss: 135.6828 | Time elapsed: 0.07 min\nStep 200 | Loss: 121.7796 | Time elapsed: 0.13 min\nStep 300 | Loss: 98.6115 | Time elapsed: 0.20 min\nStep 400 | Loss: 3108.3001 | Time elapsed: 0.26 min\nStep 500 | Loss: 2133.8837 | Time elapsed: 0.33 min\nStep 600 | Loss: 2319.4524 | Time elapsed: 0.39 min\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3291760358.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m trained_params, final_hidden = train_rnn(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mY_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/200497597.py\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(X_data, Y_data, params, hypers, char_to_ix, ix_to_char, print_every, start_text)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0my_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h_last'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Wxh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Why'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bxh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bhy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1484857455.py\u001b[0m in \u001b[0;36mrnn_forward_backward\u001b[0;34m(x_seq, y_seq, h_prev, params, vocab_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mdbhy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdh_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mdtanh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m  \u001b[0;31m# Derivative of tanh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_from_c_func_and_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \"\"\"\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17}]}