{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12201425,"sourceType":"datasetVersion","datasetId":7685774}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Character-Level RNN: Text Generation from Asimov's *Foundation*\n\nüß† Goal:\nBuild a character-level RNN from scratch using NumPy to model language patterns in Isaac Asimov's *Foundation*.\nThe trained model will generate Asimov-style text one character at a time.\n\nüîÅ Plan:\n\n1. üìñ Load and preprocess text\n   - Read *Foundation* text\n   - Create character-to-index (char2idx) and index-to-character (idx2char) mappings\n   - Encode text as integer sequence\n\n2. üß± Initialize RNN model\n   - Parameters: Wxh, Whh, Why, bh, by\n   - Hidden state size: e.g., 100\n\n3. üîÑ Forward and Backward Pass\n   - Implement one time-step forward: h_t = tanh(Wxh¬∑x_t + Whh¬∑h_{t-1} + bh)\n   - Predict next char logits: y_t = Why¬∑h_t + by\n   - Use softmax + cross-entropy loss\n   - Backpropagate gradients through time (BPTT)\n   - Apply gradient clipping\n\n4. üèãÔ∏è‚Äç‚ôÇÔ∏è Training Loop\n   - Slide a window over text with fixed sequence length (e.g., 25 chars)\n   - Compute loss and gradients\n   - Update parameters via SGD\n\n5. üß™ Sampling Function\n   - Start from a seed character\n   - Sample next character from softmax distribution\n   - Repeat for N characters\n\n6. üìâ Monitoring\n   - Print loss every N steps\n   - Print sample text every 1000 iterations\n\n7. üöÄ [Optional] Try with different corpora (e.g., Sanskrit, Shakespeare)\n\nThis notebook is a stepping stone toward building a Sanskrit name generator using character-level RNNs.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:25:55.498149Z","iopub.execute_input":"2025-06-21T05:25:55.498424Z","iopub.status.idle":"2025-06-21T05:25:55.505117Z","shell.execute_reply.started":"2025-06-21T05:25:55.498398Z","shell.execute_reply":"2025-06-21T05:25:55.504552Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ===========================\n# üîÅ RNN FORWARD + BACKWARD MODULE\n# Contains:\n# - rnn_forward_backward(): forward and backward pass of a simple RNN\n# - update_params(): SGD update with optional gradient clipping\n# ===========================\n\n\ndef rnn_forward_backward(x_seq, y_seq, h_prev, params, vocab_size):\n    \"\"\"\n    Perform a forward and backward pass of a vanilla RNN.\n    \n    Args:\n        x_seq (list[int]): Sequence of input character indices.\n        y_seq (list[int]): Sequence of target character indices.\n        h_prev (np.ndarray): Hidden state from previous batch.\n        params (dict): Dictionary of model parameters.\n        vocab_size (int): Size of vocabulary.\n        \n    Returns:\n        dict: Gradients for parameters, final hidden state, and total loss.\n    \"\"\"\n    Wxh, Whh, Why = params['Wxh'], params['Whh'], params['Why']\n    bxh, bhy = params['bxh'], params['bhy']\n\n    seq_len = len(x_seq)\n    h, y_pred, p, X = {}, {}, {}, {}\n    h[-1] = h_prev\n    loss = 0\n\n    # FORWARD PASS\n    for t in range(seq_len):\n        X[t] = one_hot_encode([x_seq[t]], vocab_size).T  # Shape: (vocab_size, 1)\n        h[t] = np.tanh(np.dot(Wxh, X[t]) + np.dot(Whh, h[t - 1]) + bxh)\n        y_pred[t] = np.dot(Why, h[t]) + bhy\n        p[t] = np.exp(y_pred[t]) / np.sum(np.exp(y_pred[t]))  # softmax\n        loss += -np.log(p[t][y_seq[t], 0])\n\n    # BACKWARD PASS\n    dWxh = np.zeros_like(Wxh)\n    dWhh = np.zeros_like(Whh)\n    dWhy = np.zeros_like(Why)\n    dbxh = np.zeros_like(bxh)\n    dbhy = np.zeros_like(bhy)\n    dh_next = np.zeros_like(h[0])\n\n    for t in reversed(range(seq_len)):\n        dy = p[t].copy()\n        dy[y_seq[t]] -= 1  # Gradient of cross-entropy loss\n        dWhy += np.dot(dy, h[t].T)\n        dbhy += dy\n\n        dh = np.dot(Why.T, dy) + dh_next\n        dtanh = (1 - h[t] ** 2) * dh  # Derivative of tanh\n        dWxh += np.dot(dtanh, X[t].T)\n        dWhh += np.dot(dtanh, h[t - 1].T)\n        dbxh += dtanh\n        dh_next = np.dot(Whh.T, dtanh)\n\n    # Optional: Gradient clipping\n    for dparam in [dWxh, dWhh, dWhy, dbxh, dbhy]:\n        np.clip(dparam, -5, 5, out=dparam)\n\n    return {\n        'dWxh': dWxh,\n        'dWhh': dWhh,\n        'dWhy': dWhy,\n        'dbxh': dbxh,\n        'dbhy': dbhy,\n        'loss': loss,\n        'h_last': h[seq_len - 1]\n    }\n\n\ndef update_params(params, grads, learning_rate):\n    \"\"\"\n    Update RNN parameters using SGD.\n    \n    Args:\n        params (dict): Model parameters.\n        grads (dict): Corresponding gradients.\n        learning_rate (float): Learning rate.\n    \"\"\"\n    for param in params:\n        # Clip gradients to prevent exploding gradients\n        np.clip(grads[param], -5, 5, out=grads[param])\n        # Update parameters using SGD\n        params[param] -= learning_rate * grads[param]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:31:26.991608Z","iopub.execute_input":"2025-06-21T05:31:26.992126Z","iopub.status.idle":"2025-06-21T05:31:27.003125Z","shell.execute_reply.started":"2025-06-21T05:31:26.992100Z","shell.execute_reply":"2025-06-21T05:31:27.002115Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ------------------------\n# Activation Functions\n# ------------------------\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation.\"\"\"\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    \"\"\"Derivative of tanh for backpropagation.\"\"\"\n    return 1.0 - np.tanh(x) ** 2\n\ndef softmax(x):\n    \"\"\"Numerically stable softmax.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / np.sum(e_x)\n\ndef softmax_with_temperature(x, temperature=1.0):\n    \"\"\"Softmax with temperature scaling.\"\"\"\n    scaled_x = x / temperature\n    e_x = np.exp(scaled_x - np.max(scaled_x))  # numerical stability\n    return e_x / np.sum(e_x)\n\n# ------------------------\n# One-Hot Encoding Utility\n# ------------------------\n\ndef one_hot_encode(indices, vocab_size):\n    \"\"\"\n    Convert list of indices to one-hot encoded 2D array.\n    Each column represents one-hot encoding of a character.\n    \"\"\"\n    result = np.zeros((vocab_size, len(indices)))\n    for i, idx in enumerate(indices):\n        result[idx, i] = 1\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:31:18.764054Z","iopub.execute_input":"2025-06-21T05:31:18.764674Z","iopub.status.idle":"2025-06-21T05:31:18.770842Z","shell.execute_reply.started":"2025-06-21T05:31:18.764649Z","shell.execute_reply":"2025-06-21T05:31:18.769900Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def initialize_rnn(hidden_size, vocab_size, seed=42):\n    \"\"\"\n    Initialize RNN model parameters and training hyperparameters.\n\n    Args:\n        hidden_size (int): Number of neurons in the hidden layer.\n        vocab_size (int): Size of the vocabulary (input and output dimension).\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        params (dict): Dictionary of RNN weights and biases.\n        hypers (dict): Dictionary of training hyperparameters.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Xavier-like initialization with small random values\n    params = {\n        'Wxh': np.random.randn(hidden_size, vocab_size) * 0.01,       # Input-to-hidden weights\n        'Whh': np.random.randn(hidden_size, hidden_size) * 0.01,      # Hidden-to-hidden weights\n        'Why': np.random.randn(vocab_size, hidden_size) * 0.01,       # Hidden-to-output weights\n        'bxh': np.zeros((hidden_size, 1)),                            # Hidden layer bias\n        'bhy': np.zeros((vocab_size, 1)),                             # Output layer bias\n    }\n\n    # Training hyperparameters\n    hypers = {\n        'alpha': 5e-3,     # Learning rate (was tuned earlier)\n        'seq_length': 50,  # Number of time steps per sequence\n        'steps': 10000,    # Total number of training steps\n        'print_every': 100,  # Interval for printing loss\n        'grad_clip': 5.0   # Gradient clipping threshold (optional to apply)\n    }\n\n    return params, hypers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:31:14.226679Z","iopub.execute_input":"2025-06-21T05:31:14.226996Z","iopub.status.idle":"2025-06-21T05:31:14.232782Z","shell.execute_reply.started":"2025-06-21T05:31:14.226972Z","shell.execute_reply":"2025-06-21T05:31:14.231971Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ===========================\n# üì¶ DATA PREP & BATCHING MODULE\n# Contains:\n# - get_vocab_mappings()\n# - encode_data()\n# - create_training_sequences()\n# - batch_generator()\n# ===========================\n\nimport numpy as np\n\ndef get_vocab_mappings(data):\n    \"\"\"\n    Builds vocabulary and mapping dictionaries from input text data.\n    \n    Args:\n        data (str): Entire text corpus\n    \n    Returns:\n        Tuple[dict, dict, int]: char_to_ix, ix_to_char, vocab_size\n    \"\"\"\n    chars = sorted(list(set(data)))\n    vocab_size = len(chars)\n    char_to_ix = {ch: i for i, ch in enumerate(chars)}\n    ix_to_char = {i: ch for ch, i in char_to_ix.items()}\n    return char_to_ix, ix_to_char, vocab_size\n\ndef encode_data(data, char_to_ix):\n    \"\"\"\n    Encodes text data into a list of integer indices.\n    \n    Args:\n        data (str): Raw text data\n        char_to_ix (dict): Character-to-index mapping\n    \n    Returns:\n        List[int]: Encoded text as indices\n    \"\"\"\n    return [char_to_ix[ch] for ch in data]\n\ndef create_training_sequences(data_ix, seq_length=50, step=1):\n    \"\"\"\n    Splits encoded data into overlapping sequences of length `seq_length`.\n    \n    Args:\n        data_ix (List[int]): Encoded data indices\n        seq_length (int): Length of each input sequence\n        step (int): Stride between sequences\n    \n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Input and target sequences\n    \"\"\"\n    X = []\n    Y = []\n    for i in range(0, len(data_ix) - seq_length, step):\n        X.append(data_ix[i : i + seq_length])\n        Y.append(data_ix[i + 1 : i + seq_length + 1])\n    return np.array(X), np.array(Y)\n\ndef batch_generator(X_data, Y_data, batch_size):\n    \"\"\"\n    Yields mini-batches of input-target pairs.\n    \n    Args:\n        X_data (np.ndarray): Input sequences\n        Y_data (np.ndarray): Target sequences\n        batch_size (int): Number of sequences per batch\n    \n    Yields:\n        Tuple[np.ndarray, np.ndarray]: Mini-batches\n    \"\"\"\n    total_samples = len(X_data)\n    for i in range(0, total_samples, batch_size):\n        X_batch = X_data[i:i + batch_size]\n        Y_batch = Y_data[i:i + batch_size]\n        yield X_batch, Y_batch\n\n# ‚úÖ Example usage:\n# char_to_ix, ix_to_char, vocab_size = get_vocab_mappings(data)\n# data_ix = encode_data(data, char_to_ix)\n# X_data, Y_data = create_training_sequences(data_ix, seq_length=50)\n# for X_batch, Y_batch in batch_generator(X_data, Y_data, batch_size=64):\n#     ...  # pass to training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:31:08.632833Z","iopub.execute_input":"2025-06-21T05:31:08.633519Z","iopub.status.idle":"2025-06-21T05:31:08.642065Z","shell.execute_reply.started":"2025-06-21T05:31:08.633491Z","shell.execute_reply":"2025-06-21T05:31:08.640951Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ===========================\n# üöÇ TRAINING + SAMPLING LOOP\n# Contains:\n# - train_rnn(): main training loop\n# - sample_rnn(): generate text using trained model\n# ===========================\n\nimport time\n\ndef train_rnn(data_ix, params, hypers, char_to_ix, ix_to_char, print_every=100, start_text='A'):\n    \"\"\"\n    Trains the RNN on input data using SGD.\n    \n    Args:\n        data_ix (List[int]): Encoded character indices from text\n        params (dict): Model parameters\n        hypers (dict): Hyperparameters\n        char_to_ix (dict): Character to index mapping\n        ix_to_char (dict): Index to character mapping\n        print_every (int): Print loss and sample text every `print_every` steps\n        start_text (str): Seed text for sampling during training\n    \"\"\"\n    vocab_size = len(char_to_ix)\n    seq_length = hypers['seq_length']\n    learning_rate = hypers['alpha']\n    steps = hypers['steps']\n    grad_clip = hypers['grad_clip']\n    \n    X_data, Y_data = create_training_sequences(data_ix, seq_length)\n    h_prev = np.zeros((params['Whh'].shape[0], 1))\n    \n    start_time = time.time()\n    \n    for step in range(steps):\n        i = step % len(X_data)\n        x_seq = X_data[i]\n        y_seq = Y_data[i]\n        \n        result = rnn_forward_backward(x_seq, y_seq, h_prev, params, vocab_size)\n        h_prev = result['h_last']\n        grads = {k: result['d' + k] for k in ['Wxh', 'Whh', 'Why', 'bxh', 'bhy']}\n        update_params(params, grads, learning_rate)\n        \n        if step % print_every == 0:\n            elapsed = (time.time() - start_time) / 60\n            print(f\"Step {step} | Loss: {result['loss']:.4f} | Time elapsed: {elapsed:.2f} min\")\n            sample_text = sample_rnn(char_to_ix[start_text], h_prev, params, ix_to_char, n=300)\n            print(\"---- Sample ----\")\n            print(sample_text)\n            print(\"----------------\\n\")\n\ndef sample_rnn(seed_ix, h_prev, params, ix_to_char, n=200, temperature=1.0):\n    \"\"\"\n    Generates a text sequence using the trained RNN.\n    \n    Args:\n        seed_ix (int): Index of the starting character\n        h_prev (np.ndarray): Initial hidden state\n        params (dict): RNN model parameters\n        ix_to_char (dict): Mapping from index to character\n        n (int): Number of characters to sample\n        temperature (float): Sampling temperature\n    \n    Returns:\n        str: Generated text\n    \"\"\"\n    Wxh, Whh, Why = params['Wxh'], params['Whh'], params['Why']\n    bxh, bhy = params['bxh'], params['bhy']\n    \n    vocab_size = Why.shape[0]\n    h = h_prev\n    x = np.zeros((vocab_size, 1))\n    x[seed_ix] = 1\n\n    generated = []\n\n    for _ in range(n):\n        h = tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bxh)\n        y = np.dot(Why, h) + bhy\n        p = softmax_with_temperature(y, temperature)\n        idx = np.random.choice(range(vocab_size), p=p.ravel())\n\n        x = np.zeros((vocab_size, 1))\n        x[idx] = 1\n        generated.append(ix_to_char[idx])\n\n    return ''.join(generated)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:31:02.744688Z","iopub.execute_input":"2025-06-21T05:31:02.745354Z","iopub.status.idle":"2025-06-21T05:31:02.795549Z","shell.execute_reply.started":"2025-06-21T05:31:02.745326Z","shell.execute_reply":"2025-06-21T05:31:02.794601Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Step 1: Load and preprocess text\nimport numpy as np\n\n# Load cleaned Asimov corpus\nwith open(\"/kaggle/input/asimov/asimov_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n    data = f.read()\n\n# Get all unique characters in the text\nchars = sorted(list(set(data)))\nvocab_size = len(chars)\n\n# Create character-to-index and index-to-character mappings\nchar_to_ix = {ch: i for i, ch in enumerate(chars)}\nix_to_char = {i: ch for ch, i in char_to_ix.items()}\n\n# Encode the entire text as a list of character indices\ndata_ix = [char_to_ix[ch] for ch in data]\n\n# Print some basic stats\nprint(f\"Total characters: {len(data)}\")\nprint(f\"Unique characters: {vocab_size}\")\nprint(f\"Sample char_to_ix: {list(char_to_ix.items())[:10]}\")\nprint(f\"Encoded text (first 20 indices): {data_ix[:20]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:30:53.517923Z","iopub.execute_input":"2025-06-21T05:30:53.518208Z","iopub.status.idle":"2025-06-21T05:30:54.259202Z","shell.execute_reply.started":"2025-06-21T05:30:53.518169Z","shell.execute_reply":"2025-06-21T05:30:54.258472Z"}},"outputs":[{"name":"stdout","text":"Total characters: 10925424\nUnique characters: 105\nSample char_to_ix: [(' ', 0), ('!', 1), ('\"', 2), ('#', 3), ('$', 4), ('%', 5), ('&', 6), (\"'\", 7), ('(', 8), (')', 9)]\nEncoded text (first 20 indices): [31, 72, 80, 68, 75, 81, 67, 68, 0, 39, 0, 68, 61, 82, 65, 0, 83, 78, 69, 80]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Initialize model and hyperparameters\nhidden_size = 128\nparams, hypers = initialize_rnn(hidden_size, vocab_size)\n\n# Generate training data\nX_data, Y_data = create_training_sequences(data_ix, seq_length=hypers['seq_length'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:31:43.679562Z","iopub.execute_input":"2025-06-21T05:31:43.680098Z","iopub.status.idle":"2025-06-21T05:33:55.588349Z","shell.execute_reply.started":"2025-06-21T05:31:43.680072Z","shell.execute_reply":"2025-06-21T05:33:55.587668Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Train the model\ntrained_params, final_hidden = train_rnn(\n    data_ix,\n    params,\n    hypers,\n    char_to_ix,\n    ix_to_char,\n    print_every=10,\n    start_text=\"The \"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:37:09.669405Z","iopub.execute_input":"2025-06-21T05:37:09.669762Z"}},"outputs":[],"execution_count":null}]}